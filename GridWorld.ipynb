{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 ). Estimatation the value function for each of the states solving the system of Bellman equations explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellman Equations Solution for Value Function:\n",
      "[[ 2.14651952  4.7043962   2.00431869  1.1365939   1.51985638]\n",
      " [ 1.09317327  1.74971384  1.12024357  0.65120534  0.43166061]\n",
      " [ 0.13974397  0.44940312  0.31157719  0.05341914 -0.25888819]\n",
      " [-0.56760891 -0.30881131 -0.31116189 -0.47897166 -0.78993129]\n",
      " [-1.12694021 -0.87089049 -0.83394953 -0.96904876 -1.27191954]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_world_size = 5  # Grid size is 5x5\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.zeros((grid_world_size, grid_world_size))  # Initialize rewards matrix with zeros\n",
    "rewards[0, 1] = 5   # Set reward for the blue square\n",
    "rewards[0, 4] = 2.5  # Set reward for the green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "# Each entry is a tuple (reward, next_state)\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (3, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (3, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Actions represent movements: up, down, left, right\n",
    "\n",
    "# Create the system of equations\n",
    "num_states = grid_world_size * grid_world_size  # Total number of states in the grid\n",
    "K = np.zeros((num_states, num_states))  # Initialize matrix A for Bellman equations\n",
    "L = np.zeros(num_states)  # Initialize vector b for Bellman equations\n",
    "\n",
    "# Function to get the state index from (i, j)\n",
    "def get_state_index(i, j):\n",
    "    return i * grid_world_size + j  # Convert 2D grid coordinates to 1D state index\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in transitions:\n",
    "        # Special squares with specific transitions\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))  # Randomly choose one of the possible transitions\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        # Normal transitions\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_world_size or next_state[1] < 0 or next_state[1] >= grid_world_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (0, next_state)  # Normal move\n",
    "\n",
    "# Build the matrix K and vector L\n",
    "for i in range(grid_world_size):\n",
    "    for j in range(grid_world_size):\n",
    "        state_idx = get_state_index(i, j)  # Get 1D state index for (i, j)\n",
    "        if (i, j) in transitions:\n",
    "            for action in actions:\n",
    "                reward, next_state = get_next_state_reward((i, j), action)\n",
    "                next_state_idx = get_state_index(next_state[0], next_state[1])\n",
    "                K[state_idx, next_state_idx] -= gamma / 4  # Adjust matrix A for special transitions\n",
    "            L[state_idx] = rewards[i, j]  # Set corresponding reward in vector b\n",
    "        else:\n",
    "            for action in actions:\n",
    "                reward, next_state = get_next_state_reward((i, j), action)\n",
    "                next_state_idx = get_state_index(next_state[0], next_state[1])\n",
    "                K[state_idx, next_state_idx] -= gamma / 4  # Adjust matrix A for normal transitions\n",
    "                L[state_idx] += reward / 4  # Average reward contribution from all actions\n",
    "\n",
    "# Add identity matrix to A for the equation V(s) = RHS\n",
    "K += np.eye(num_states)  # Ensure diagonal dominance in matrix A\n",
    "\n",
    "# Solve the system of equations\n",
    "V_bellman = np.linalg.solve(K, L)  # Solve for value function using linear algebra\n",
    "\n",
    "# Reshape the value function to grid form\n",
    "V_bellman_grid = V_bellman.reshape((grid_world_size, grid_world_size))  # Convert 1D value array to 2D grid\n",
    "\n",
    "print(\"Bellman Equations Solution for Value Function:\")\n",
    "print(V_bellman_grid)  # Display the computed value function for the grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 ). Estimatation the value function for each of the states using iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative Policy Evaluation for Value Function:\n",
      "[[ 2.17007327  4.73281051  2.06948333  1.26450762  1.77834473]\n",
      " [ 1.11717144  1.78130365  1.17331876  0.73842046  0.56172351]\n",
      " [ 0.16189453  0.47706555  0.3512111   0.10971459 -0.18689699]\n",
      " [-0.54789828 -0.28556322 -0.28117951 -0.44064938 -0.74503334]\n",
      " [-1.10878884 -0.85020237 -0.80876952 -0.93873288 -1.237955  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5  # Grid size is 5x5\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.zeros((grid_size, grid_size))  # Initialize rewards matrix with zeros\n",
    "rewards[0, 1] = 5   # Set reward for the blue square\n",
    "rewards[0, 4] = 2.5  # Set reward for the green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "# Each entry is a tuple (reward, next_state)\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (3, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (3, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Actions represent movements: up, down, left, right\n",
    "\n",
    "# Initialize the value function\n",
    "V = np.zeros((grid_size, grid_size))  # Initialize value function with zeros\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in transitions:\n",
    "        # Special squares with specific transitions\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))  # Randomly choose one of the possible transitions\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        # Normal transitions\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (0, next_state)  # Normal move\n",
    "\n",
    "# Iterative Policy Evaluation\n",
    "def iterative_policy_evaluation(V, policy, theta=1e-4):\n",
    "    while True:\n",
    "        delta = 0  # Initialize delta for convergence check\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]  # Store the current value\n",
    "                new_v = 0  # Initialize new value\n",
    "                for action in actions:\n",
    "                    reward, next_state = get_next_state_reward((i, j), action)\n",
    "                    new_v += 0.25 * (reward + gamma * V[next_state])  # Update new value with action probability\n",
    "                V[i, j] = new_v  # Update the value function\n",
    "                delta = max(delta, abs(v - new_v))  # Update delta\n",
    "        if delta < theta:  # Check for convergence\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Initialize policy (equal probability for all actions)\n",
    "policy = np.ones((grid_size, grid_size, len(actions))) / len(actions)\n",
    "\n",
    "# Perform Iterative Policy Evaluation\n",
    "V_iterative = iterative_policy_evaluation(V.copy(), policy)\n",
    "\n",
    "print(\"Iterative Policy Evaluation for Value Function:\")\n",
    "print(V_iterative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 ). Estimatation the value function for each of the states using value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration Solution for Value Function:\n",
      "[[20.99700533 22.10219208 20.99708247 19.94722835 19.60219208]\n",
      " [19.94715507 20.99708247 19.94722835 18.94986693 18.62208247]\n",
      " [18.94979731 19.94722835 18.94986693 18.00237358 17.69097835]\n",
      " [18.00230745 18.94986693 18.00237358 17.10225491 16.80642943]\n",
      " [17.10219208 18.00237358 17.10225491 16.24714216 15.96610796]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5  # Grid size is 5x5\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.zeros((grid_size, grid_size))  # Initialize rewards matrix with zeros\n",
    "rewards[0, 1] = 5   # Set reward for the blue square\n",
    "rewards[0, 4] = 2.5  # Set reward for the green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "# Each entry is a tuple (reward, next_state)\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (3, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (3, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Actions represent movements: up, down, left, right\n",
    "\n",
    "# Initialize the value function\n",
    "V = np.zeros((grid_size, grid_size))  # Initialize value function with zeros\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in transitions:\n",
    "        # Special squares with specific transitions\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))  # Randomly choose one of the possible transitions\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        # Normal transitions\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (0, next_state)  # Normal move\n",
    "\n",
    "# Value Iteration\n",
    "def value_iteration(V, theta=1e-4):\n",
    "    while True:\n",
    "        delta = 0  # Initialize delta for convergence check\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]  # Store the current value\n",
    "                max_v = -float('inf')  # Initialize maximum value for the state\n",
    "                for action in actions:\n",
    "                    reward, next_state = get_next_state_reward((i, j), action)\n",
    "                    new_v = reward + gamma * V[next_state]  # Calculate the new value\n",
    "                    if new_v > max_v:\n",
    "                        max_v = new_v  # Update maximum value\n",
    "                V[i, j] = max_v  # Update the value function\n",
    "                delta = max(delta, abs(v - max_v))  # Update delta\n",
    "        if delta < theta:  # Check for convergence\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Perform Value Iteration\n",
    "V_value_iteration = value_iteration(V.copy())\n",
    "\n",
    "print(\"Value Iteration Solution for Value Function:\")\n",
    "print(V_value_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 ). Optimal policy for the gridworld problem by explicitly solving the Bellman optimality equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function by Solving Bellman Optimality Equation:\n",
      "[[20.99734295 22.10246707 20.99734371 19.94747653 19.60246707]\n",
      " [19.9474758  20.99734371 19.94747653 18.9501027  18.62234371]\n",
      " [18.95010201 19.94747653 18.9501027  18.00259757 17.69122653]\n",
      " [18.00259691 18.9501027  18.00259757 17.10246769 16.8066652 ]\n",
      " [17.10246707 18.00259757 17.10246769 16.2473443  15.96633194]]\n",
      "\n",
      "Optimal Policy (0=up, 1=down, 2=left, 3=right):\n",
      "[[3 0 2 2 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5  # Grid size is 5x5\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.zeros((grid_size, grid_size))  # Initialize rewards matrix with zeros\n",
    "rewards[0, 1] = 5   # Set reward for the blue square\n",
    "rewards[0, 4] = 2.5  # Set reward for the green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "# Each entry is a tuple (reward, next_state)\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (3, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (3, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Actions represent movements: up, down, left, right\n",
    "action_indices = {(-1, 0): 0, (1, 0): 1, (0, -1): 2, (0, 1): 3}  # Map actions to indices\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in transitions:\n",
    "        # Special squares with specific transitions\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))  # Randomly choose one of the possible transitions\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        # Normal transitions\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (0, next_state)  # Normal move\n",
    "\n",
    "# Bellman Optimality Equation\n",
    "def solve_bellman_optimality(V, theta=1e-6):\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)  # Initialize policy with zeros\n",
    "    while True:\n",
    "        delta = 0  # Initialize delta for convergence check\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]  # Store the current value\n",
    "                max_v = -float('inf')  # Initialize maximum value\n",
    "                best_action = None\n",
    "                for action in actions:\n",
    "                    reward, next_state = get_next_state_reward((i, j), action)\n",
    "                    next_value = reward + gamma * V[next_state]  # Calculate the next value\n",
    "                    if next_value > max_v:\n",
    "                        max_v = next_value  # Update maximum value\n",
    "                        best_action = action  # Update best action\n",
    "                V[i, j] = max_v  # Update value function\n",
    "                policy[i, j] = action_indices[best_action]  # Update policy with the best action index\n",
    "                delta = max(delta, abs(v - max_v))  # Update delta\n",
    "        if delta < theta:  # Check for convergence\n",
    "            break\n",
    "    return V, policy\n",
    "\n",
    "# Solve the Bellman optimality equation\n",
    "V_optimal, optimal_policy = solve_bellman_optimality(np.zeros((grid_size, grid_size)))\n",
    "\n",
    "print(\"Optimal Value Function by Solving Bellman Optimality Equation:\")\n",
    "print(V_optimal)\n",
    "print(\"\\nOptimal Policy (0=up, 1=down, 2=left, 3=right):\")\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 ). Optimal policy for the gridworld problem by explicitly using policy iteration with iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function by Policy Iteration:\n",
      "[[20.99734539 22.10246905 20.9973456  19.94747832 17.1632294 ]\n",
      " [19.94747812 20.9973456  19.94747832 18.9501044  18.00259918]\n",
      " [18.95010421 19.94747832 18.9501044  18.00259918 17.10246922]\n",
      " [18.002599   18.9501044  18.00259918 17.10246922 16.24734576]\n",
      " [17.10246905 18.00259918 17.10246922 16.24734576 15.43497847]]\n",
      "\n",
      "Optimal Policy (0=up, 1=down, 2=left, 3=right):\n",
      "[[3 0 2 2 0]\n",
      " [3 0 0 0 2]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5  # Grid size is 5x5\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.zeros((grid_size, grid_size))  # Initialize rewards matrix with zeros\n",
    "rewards[0, 1] = 5   # Set reward for the blue square\n",
    "rewards[0, 4] = 2.5  # Set reward for the green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "# Each entry is a tuple (reward, next_state)\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (3, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (3, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Actions: up, down, left, right\n",
    "action_indices = {(-1, 0): 0, (1, 0): 1, (0, -1): 2, (0, 1): 3}  # Map actions to indices\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in transitions:\n",
    "        # Special squares with specific transitions\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))  # Randomly choose one of the possible transitions\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        # Normal transitions\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (0, next_state)  # Normal move\n",
    "\n",
    "# Policy Evaluation\n",
    "def policy_evaluation(policy, V, theta=1e-6):\n",
    "    while True:\n",
    "        delta = 0  # Initialize delta for convergence check\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]  # Store the current value\n",
    "                action = actions[policy[i, j]]  # Get the action from the policy\n",
    "                reward, next_state = get_next_state_reward((i, j), action)\n",
    "                V[i, j] = reward + gamma * V[next_state]  # Update the value function\n",
    "                delta = max(delta, abs(v - V[i, j]))  # Update delta\n",
    "        if delta < theta:  # Check for convergence\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Policy Improvement\n",
    "def policy_improvement(V, policy):\n",
    "    policy_stable = True  # Initialize policy stability flag\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            old_action = policy[i, j]  # Store the current policy action\n",
    "            max_v = -float('inf')  # Initialize maximum value\n",
    "            best_action = None  # Initialize best action\n",
    "            for action in actions:\n",
    "                reward, next_state = get_next_state_reward((i, j), action)\n",
    "                next_value = reward + gamma * V[next_state]  # Calculate the next value\n",
    "                if next_value > max_v:\n",
    "                    max_v = next_value  # Update maximum value\n",
    "                    best_action = action  # Update best action\n",
    "            policy[i, j] = action_indices[best_action]  # Update policy with the best action index\n",
    "            if old_action != policy[i, j]:  # Check if policy has changed\n",
    "                policy_stable = False\n",
    "    return policy, policy_stable\n",
    "\n",
    "# Policy Iteration\n",
    "def policy_iteration():\n",
    "    # Initialize the value function and policy\n",
    "    V = np.zeros((grid_size, grid_size))\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)\n",
    "\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, V)  # Evaluate the current policy\n",
    "        policy, policy_stable = policy_improvement(V, policy)  # Improve the policy\n",
    "        if policy_stable:  # Check if the policy is stable\n",
    "            break\n",
    "\n",
    "    return V, policy\n",
    "\n",
    "# Perform Policy Iteration\n",
    "optimal_value_function, optimal_policy = policy_iteration()\n",
    "\n",
    "print(\"Optimal Value Function by Policy Iteration:\")\n",
    "print(optimal_value_function)\n",
    "print(\"\\nOptimal Policy (0=up, 1=down, 2=left, 3=right):\")\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 ). Optimal policy for the gridworld problem by explicitly using policy improvement with value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function by Value Iteration:\n",
      "[[20.99734295 22.10246707 20.99734371 19.94747653 19.60246707]\n",
      " [19.9474758  20.99734371 19.94747653 18.9501027  18.62234371]\n",
      " [18.95010201 19.94747653 18.9501027  18.00259757 17.69122653]\n",
      " [18.00259691 18.9501027  18.00259757 17.10246769 16.8066652 ]\n",
      " [17.10246707 18.00259757 17.10246769 16.2473443  15.96633194]]\n",
      "\n",
      "Optimal Policy (0=up, 1=down, 2=left, 3=right):\n",
      "[[3 0 2 2 0]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]\n",
      " [3 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the gridworld size and discount factor\n",
    "grid_size = 5  # Grid size is 5x5\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    "\n",
    "# Define the rewards and transitions\n",
    "rewards = np.zeros((grid_size, grid_size))  # Initialize rewards matrix with zeros\n",
    "rewards[0, 1] = 5   # Set reward for the blue square\n",
    "rewards[0, 4] = 2.5  # Set reward for the green square\n",
    "\n",
    "# Define transitions for special squares\n",
    "# Each entry is a tuple (reward, next_state)\n",
    "transitions = {}\n",
    "transitions[(0, 1)] = [(5, (3, 2))]   # Blue -> Red\n",
    "transitions[(0, 4)] = [(2.5, (3, 2)), (2.5, (4, 4))]  # Green -> Red or Yellow\n",
    "\n",
    "# Define the four possible actions\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Actions: up, down, left, right\n",
    "action_indices = {(-1, 0): 0, (1, 0): 1, (0, -1): 2, (0, 1): 3}  # Map actions to indices\n",
    "\n",
    "# Function to get the next state and reward\n",
    "def get_next_state_reward(state, action):\n",
    "    if state in transitions:\n",
    "        # Special squares with specific transitions\n",
    "        transition = transitions[state]\n",
    "        idx = np.random.choice(len(transition))  # Randomly choose one of the possible transitions\n",
    "        return transition[idx]\n",
    "    else:\n",
    "        # Normal transitions\n",
    "        next_state = (state[0] + action[0], state[1] + action[1])\n",
    "        if next_state[0] < 0 or next_state[0] >= grid_size or next_state[1] < 0 or next_state[1] >= grid_size:\n",
    "            return (-0.5, state)  # Stepping off the grid\n",
    "        return (0, next_state)  # Normal move\n",
    "\n",
    "# Value Iteration\n",
    "def value_iteration(V, theta=1e-6):\n",
    "    while True:\n",
    "        delta = 0  # Initialize delta for convergence check\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                v = V[i, j]  # Store the current value\n",
    "                max_v = -float('inf')  # Initialize maximum value\n",
    "                for action in actions:\n",
    "                    reward, next_state = get_next_state_reward((i, j), action)\n",
    "                    new_v = reward + gamma * V[next_state]  # Calculate the new value\n",
    "                    if new_v > max_v:\n",
    "                        max_v = new_v  # Update maximum value\n",
    "                V[i, j] = max_v  # Update value function\n",
    "                delta = max(delta, abs(v - max_v))  # Update delta\n",
    "        if delta < theta:  # Check for convergence\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Policy Improvement\n",
    "def policy_improvement(V):\n",
    "    policy = np.zeros((grid_size, grid_size), dtype=int)  # Initialize policy with zeros\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            max_v = -float('inf')  # Initialize maximum value\n",
    "            best_action = None  # Initialize best action\n",
    "            for action in actions:\n",
    "                reward, next_state = get_next_state_reward((i, j), action)\n",
    "                next_value = reward + gamma * V[next_state]  # Calculate the next value\n",
    "                if next_value > max_v:\n",
    "                    max_v = next_value  # Update maximum value\n",
    "                    best_action = action  # Update best action\n",
    "            policy[i, j] = action_indices[best_action]  # Update policy with the best action index\n",
    "    return policy\n",
    "\n",
    "# Perform Value Iteration to find the optimal value function\n",
    "V_optimal = value_iteration(np.zeros((grid_size, grid_size)))\n",
    "\n",
    "# Perform Policy Improvement to find the optimal policy\n",
    "optimal_policy = policy_improvement(V_optimal)\n",
    "\n",
    "print(\"Optimal Value Function by Value Iteration:\")\n",
    "print(V_optimal)\n",
    "print(\"\\nOptimal Policy (0=up, 1=down, 2=left, 3=right):\")\n",
    "print(optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sasinda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
